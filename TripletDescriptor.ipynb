{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TripletDescriptor.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "WXH0-9YHt1BK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ]
    },
    {
      "metadata": {
        "id": "LeIGcwCfmEBx",
        "colab_type": "code",
        "outputId": "eff737bd-510e-415c-b861-c4c53303198b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "cell_type": "code",
      "source": [
        "# Clone repo\n",
        "!git clone https://github.com/MatchLab-Imperial/keras_triplet_descriptor.git\n",
        "  \n",
        "# Change directory\n",
        "%cd /content/keras_triplet_descriptor  \n",
        "\n",
        "Download data\n",
        "!wget -O hpatches_data.zip https://imperialcollegelondon.box.com/shared/static/ah40eq7cxpwq4a6l4f62efzdyt8rm3ha.zip\n",
        "\n",
        "  \n",
        "# Extract data\n",
        "!unzip -q ./hpatches_data.zip\n",
        "!rm ./hpatches_data.zip\n",
        "\n",
        "import sys\n",
        "import json\n",
        "import os\n",
        "import glob\n",
        "import time\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import cv2\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import keras\n",
        "from keras import backend as K\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, Input, Lambda, Reshape\n",
        "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization \n",
        "from keras.layers import Input, UpSampling2D, concatenate  \n",
        "\n",
        "from read_data import HPatches, DataGeneratorDesc, hpatches_sequence_folder, DenoiseHPatches, tps\n",
        "from utils import generate_desc_csv, plot_denoise, plot_triplet\n",
        "\n",
        "import csv\n",
        "\n",
        "def pushCSV(log, title, metric='acc'):\n",
        " \n",
        "  with open(title, mode='w') as file:\n",
        "    writer = csv.writer(file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
        "    writer.writerow(['epoch','loss','val_loss', metric,'val_' + metric])\n",
        "    for i in range(len(log)):\n",
        "      \n",
        "      his = log[i].history\n",
        "      writer.writerow([i+1,his['loss'], his['val_loss'], his[metric], his['val_' + metric]])\n",
        "  \n",
        "  \n",
        "  from google.colab import files\n",
        "  files.download(title)\n",
        "\n",
        "random.seed(1234)\n",
        "np.random.seed(1234)\n",
        "tf.set_random_seed(1234)\n",
        "\n",
        "hpatches_dir = './hpatches'\n",
        "splits_path = './splits.json'\n",
        "\n",
        "splits_json = json.load(open(splits_path, 'rb'))\n",
        "split = splits_json['a']\n",
        "\n",
        "train_fnames = split['train']\n",
        "test_fnames = split['test']\n",
        "\n",
        "seqs = glob.glob(hpatches_dir+'/*')\n",
        "seqs = [os.path.abspath(p) for p in seqs]   \n",
        "seqs_train = list(filter(lambda x: x.split('/')[-1] in train_fnames, seqs)) \n",
        "seqs_test = list(filter(lambda x: x.split('/')[-1] in split['test'], seqs)) "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'keras_triplet_descriptor'...\n",
            "remote: Enumerating objects: 3, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 181 (delta 0), reused 1 (delta 0), pack-reused 178\u001b[K\n",
            "Receiving objects: 100% (181/181), 149.87 MiB | 20.22 MiB/s, done.\n",
            "Resolving deltas: 100% (65/65), done.\n",
            "Checking out files: 100% (69/69), done.\n",
            "/content/keras_triplet_descriptor\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "PDIHKbG7u_Oi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Declare models"
      ]
    },
    {
      "metadata": {
        "id": "eLtUfaF1u_0F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_denoise_model(shape):\n",
        "  \n",
        "  \n",
        "  init_weights = keras.initializers.glorot_normal()\n",
        "    \n",
        "  inputs = Input(shape)\n",
        " \n",
        "  conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = init_weights)(inputs)\n",
        "  pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "  conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = init_weights)(pool1)\n",
        "  pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "  conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = init_weights)(pool2)\n",
        "  pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
        "  conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = init_weights)(pool3)\n",
        "  drop4 = Dropout(0.5)(conv4)\n",
        "  pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
        "\n",
        "  conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = init_weights)(pool4)\n",
        "  drop5 = Dropout(0.5)(conv5)\n",
        "\n",
        "  ## Now the decoder starts\n",
        "\n",
        "  up6 = Conv2D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = init_weights)(UpSampling2D(size = (2,2))(drop5))\n",
        "  merge6 = concatenate([drop4,up6], axis = 3)\n",
        "  conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = init_weights)(merge6)\n",
        "\n",
        "  up7 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = init_weights)(UpSampling2D(size = (2,2))(conv6))\n",
        "  merge7 = concatenate([conv3,up7], axis = 3)\n",
        "  conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = init_weights)(merge7)\n",
        "\n",
        "  up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = init_weights)(UpSampling2D(size = (2,2))(conv7))\n",
        "  merge8 = concatenate([conv2,up8], axis = 3)\n",
        "  conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = init_weights)(merge8)\n",
        "\n",
        "  up9 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = init_weights)(UpSampling2D(size = (2,2))(conv8))\n",
        "  merge9 = concatenate([conv1,up9], axis = 3)\n",
        "  conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = init_weights)(merge9)\n",
        "  conv10 = Conv2D(1, 3,  padding = 'same')(conv9)\n",
        "\n",
        "  model = Model(inputs = inputs, outputs = conv10)\n",
        "  \n",
        "  return model\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_baseline_DN_model(shape):\n",
        "  \n",
        "  inputs = Input(shape)\n",
        "  \n",
        "  ## Encoder starts \n",
        "  conv1 = Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)\n",
        "  pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "  \n",
        "  ## Bottleneck\n",
        "  conv2 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
        "\n",
        "  ## Now the decoder starts\n",
        "  up3 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv2))\n",
        "  merge3 = concatenate([conv1,up3], axis = -1)\n",
        "  conv3 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge3)\n",
        "    \n",
        "  conv4 = Conv2D(1, 3,  padding = 'same')(conv3)\n",
        "\n",
        "  shallow_net = Model(inputs = inputs, outputs = conv4)\n",
        "  \n",
        "  return shallow_net\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_BL_descriptor_model(shape):\n",
        "  \n",
        "  '''Architecture copies HardNet architecture'''\n",
        "  \n",
        "  init_weights = keras.initializers.glorot_normal()\n",
        "  reg = keras.regularizers.l1(0.01)\n",
        "  \n",
        "  descriptor_model = Sequential()\n",
        "  descriptor_model.add(Conv2D(32, 3, padding='same', input_shape=shape, use_bias = True, kernel_initializer=init_weights))\n",
        "  descriptor_model.add(BatchNormalization(axis = -1))\n",
        "  descriptor_model.add(Activation('relu'))\n",
        "\n",
        "  descriptor_model.add(Conv2D(32, 3, padding='same', use_bias = True, kernel_initializer=init_weights))\n",
        "  descriptor_model.add(BatchNormalization(axis = -1))\n",
        "  descriptor_model.add(Activation('relu'))\n",
        "\n",
        "  descriptor_model.add(Conv2D(64, 3, padding='same', strides=2, use_bias = True, kernel_initializer=init_weights))\n",
        "  descriptor_model.add(BatchNormalization(axis = -1))\n",
        "  descriptor_model.add(Activation('relu'))\n",
        "\n",
        "  descriptor_model.add(Conv2D(64, 3, padding='same', use_bias = True, kernel_initializer=init_weights))\n",
        "  descriptor_model.add(BatchNormalization(axis = -1))\n",
        "  descriptor_model.add(Activation('relu'))\n",
        "\n",
        "  descriptor_model.add(Conv2D(128, 3, padding='same', strides=2,  use_bias = True, kernel_initializer=init_weights))\n",
        "  descriptor_model.add(BatchNormalization(axis = -1))\n",
        "  descriptor_model.add(Activation('relu'))\n",
        "\n",
        "  descriptor_model.add(Conv2D(128, 3, padding='same', use_bias = True, kernel_initializer=init_weights))\n",
        "  descriptor_model.add(BatchNormalization(axis = -1))\n",
        "  descriptor_model.add(Activation('relu'))\n",
        "  descriptor_model.add(Dropout(0.3))\n",
        "\n",
        "  descriptor_model.add(Conv2D(128, 8, padding='valid', use_bias = True, kernel_initializer=init_weights))\n",
        "  \n",
        "  # Final descriptor reshape\n",
        "  descriptor_model.add(Reshape((128,)))\n",
        "  \n",
        "  return descriptor_model\n",
        "\n",
        "\n",
        "def get_descriptor_model(shape):\n",
        "  model = keras.applications.xception.Xception(include_top=False, weights='imagenet', input_tensor=None, input_shape=(75, 75, 3), pooling=None)\n",
        "  \n",
        "  newInput = Input(batch_shape=(None, 32, 32, 1))\n",
        "  resizedImg = Lambda(lambda image: tf.image.resize_images(image, (75, 75)))(newInput)\n",
        "  resizedRGB = concatenate([resizedImg,resizedImg,resizedImg])\n",
        "  newOutputs = model(resizedRGB)\n",
        "  model = Model(newInput, newOutputs)\n",
        "\n",
        "  # Freeze all the layers\n",
        "  for layer in model.layers[:]:\n",
        "      layer.trainable = False\n",
        "\n",
        "  # Add Dense layer to classify on CIFAR100\n",
        "  output = model.output\n",
        "  output = Dense(units=128, activation='softmax')(output)\n",
        "  model = Model(model.input, output)\n",
        "  \n",
        "  return model\n",
        "  \n",
        "def triplet_loss(x):\n",
        "  \n",
        "  output_dim = 128\n",
        "  a, p, n = x\n",
        "  _alpha = 1.0\n",
        "  positive_distance = K.mean(K.square(a - p), axis=-1)\n",
        "  negative_distance = K.mean(K.square(a - n), axis=-1)\n",
        "  \n",
        "  return K.expand_dims(K.maximum(0.0, positive_distance - negative_distance + _alpha), axis = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eCvgrc7svM3w",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Train Baseline models"
      ]
    },
    {
      "metadata": {
        "id": "RYx4sJ6Cxa5w",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Denoiser"
      ]
    },
    {
      "metadata": {
        "id": "HrwzUqoJvHa2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "denoise_generator = DenoiseHPatches(random.sample(seqs_train, 3), batch_size=50)   # Takes 3 sample sequences and outputs clean as labe\n",
        "denoise_generator_val = DenoiseHPatches(random.sample(seqs_test, 1), batch_size=50)    # Takes 1 sample sequence and outputs clean as label\n",
        "\n",
        "shape = (32, 32, 1)\n",
        "denoise_model = get_baseline_DN_model(shape)\n",
        "\n",
        "\n",
        "adam = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
        "denoise_log = []\n",
        "denoise_model.compile(loss='mean_absolute_error', optimizer=adam, metrics=['acc'])\n",
        "\n",
        "epochs = 30\n",
        "### Use a loop to save for each epoch the weights in an external website in\n",
        "### case colab stops. Every time you call fit/fit_generator the weigths are NOT\n",
        "### reset, so e.g. calling 5 times fit(epochs=1) behave as fit(epochs=5)\n",
        "for e in range(epochs):\n",
        "  print(\"\\n/////          \" + str(e) + \"      ////////    \")\n",
        "  denoise_history = denoise_model.fit_generator(generator=denoise_generator, \n",
        "                                                epochs=1, verbose=1, \n",
        "                                                validation_data=denoise_generator_val)\n",
        "  denoise_log.append(denoise_history)\n",
        "  ### Saves optimizer and weights\n",
        "  denoise_model.save('denoise.h5') \n",
        "  ### Uploads files to external hosting\n",
        "  !curl -F \"file=@denoise.h5\" https://file.io\n",
        "    \n",
        "del denoise_generator\n",
        "del denoise_generator_val"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "15-s3w50xcFP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Descriptor"
      ]
    },
    {
      "metadata": {
        "id": "q7rh2-eQxOAn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Lambda\n",
        "shape = (32, 32, 1)\n",
        "xa = Input(shape=shape, name='a')\n",
        "xp = Input(shape=shape, name='p')\n",
        "xn = Input(shape=shape, name='n')\n",
        "descriptor_model = get_BL_descriptor_model(shape)\n",
        "ea = descriptor_model(xa)\n",
        "ep = descriptor_model(xp)\n",
        "en = descriptor_model(xn)\n",
        "\n",
        "loss = Lambda(triplet_loss)([ea, ep, en])\n",
        "\n",
        "desc_log = []\n",
        "\n",
        "descriptor_model_trip = Model(inputs=[xa, xp, xn], outputs=loss)\n",
        "# sgd = keras.optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=0.0)\n",
        "adam = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
        "descriptor_model_trip.compile(loss='mean_absolute_error', optimizer=adam, metrics=['acc'])\n",
        "\n",
        "### Descriptor loading and training\n",
        "# Loading images\n",
        "hPatches = HPatches(train_fnames=train_fnames, test_fnames=test_fnames,\n",
        "                    denoise_model=None, use_clean=True)\n",
        "# Creating training generator\n",
        "training_generator = DataGeneratorDesc(*hPatches.read_image_file(hpatches_dir, train=1), num_triplets=100000)\n",
        "# Creating validation generator\n",
        "val_generator = DataGeneratorDesc(*hPatches.read_image_file(hpatches_dir, train=0), num_triplets=10000)\n",
        "plot_triplet(training_generator)\n",
        "\n",
        "epochs = 30\n",
        "### Use a loop to save for each epoch the weights in an external website in\n",
        "### case colab stops. Every time you call fit/fit_generator the weigths are NOT\n",
        "### reset, so e.g. calling 5 times fit(epochs=1) behave as fit(epochs=5)\n",
        "for e in range(epochs):\n",
        "  print(\"\\n/////          \" + str(e) + \"      ////////    \")\n",
        "  denoise_history = denoise_model.fit_generator(generator=denoise_generator, \n",
        "                                                epochs=1, verbose=1, \n",
        "                                                validation_data=denoise_generator_val)\n",
        "  denoise_log.append(denoise_history)\n",
        "  ### Saves optimizer and weights\n",
        "  denoise_model.save('denoise.h5') \n",
        "  ### Uploads files to external hosting\n",
        "  !curl -F \"file=@denoise.h5\" https://file.io\n",
        "\n",
        "\n",
        "del hPatches\n",
        "del training_generator\n",
        "del val_generator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IuNsxTBY1MxY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Run benchmark"
      ]
    },
    {
      "metadata": {
        "id": "Hq4VzDR-wvge",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "generate_desc_csv(descriptor_model, seqs_test, denoise_model=denoise_model, use_clean=False)\n",
        "\n",
        "!python ./hpatches-benchmark/hpatches_eval.py --descr-name=custom --descr-dir=/content/DL_CW_HPatches/out/ --task=verification --delimiter=\";\"\n",
        "!python ./hpatches-benchmark/hpatches_results.py --descr=custom --results-dir=./hpatches-benchmark/results/ --task=verification\n",
        "\n",
        "!python ./hpatches-benchmark/hpatches_eval.py --descr-name=custom --descr-dir=/content/DL_CW_HPatches/out/ --task=matching --delimiter=\";\"\n",
        "!python ./hpatches-benchmark/hpatches_results.py --descr=custom --results-dir=./hpatches-benchmark/results/ --task=matching\n",
        "\n",
        "!python ./hpatches-benchmark/hpatches_eval.py --descr-name=custom --descr-dir=/content/DL_CW_HPatches/out/ --task=retrieval --delimiter=\";\"\n",
        "!python ./hpatches-benchmark/hpatches_results.py --descr=custom --results-dir=./hpatches-benchmark/results/ --task=retrieval"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3-bmEIimmTv5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Import what's necessary\n",
        "Fix seeds\n",
        "Import and split data\n"
      ]
    },
    {
      "metadata": {
        "id": "q3zSuKKRt9IA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Improved models"
      ]
    },
    {
      "metadata": {
        "id": "BFP9UM6tx06E",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Setup"
      ]
    },
    {
      "metadata": {
        "id": "55Au5ZuYxz7x",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!cd ..\n",
        "!rm -r /content/keras_triplet_descriptor\n",
        "\n",
        "# Clone repo\n",
        "!git clone https://github.com/ms6616/DL_CW_HPatches.git\n",
        "  \n",
        "# Change directory\n",
        "%cd /content/DL_CW_HPatches  \n",
        "\n",
        "# Download data\n",
        "!wget -O hpatches_data.zip https://imperialcollegelondon.box.com/shared/static/ah40eq7cxpwq4a6l4f62efzdyt8rm3ha.zip\n",
        "\n",
        "  \n",
        "# Extract data\n",
        "!unzip -q ./hpatches_data.zip\n",
        "!rm ./hpatches_data.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s_9uNazCxveG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Denoiser"
      ]
    },
    {
      "metadata": {
        "id": "CAfpObmHxwWx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "denoise_generator = DenoiseHPatches(random.sample(seqs_train, 6), batch_size=50)   # Takes 6 sample sequences and outputs clean as labe\n",
        "denoise_generator_val = DenoiseHPatches(random.sample(seqs_test, 1), batch_size=50)    # Takes 1 sample sequence and outputs clean as label\n",
        "\n",
        "shape = (32, 32, 1)\n",
        "denoise_model = get_denoise_model(shape)\n",
        "\n",
        "\n",
        "adam = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
        "denoise_log = []\n",
        "denoise_model.compile(loss='mean_absolute_error', optimizer=adam, metrics=['acc'])\n",
        "\n",
        "epochs = 30\n",
        "### Use a loop to save for each epoch the weights in an external website in\n",
        "### case colab stops. Every time you call fit/fit_generator the weigths are NOT\n",
        "### reset, so e.g. calling 5 times fit(epochs=1) behave as fit(epochs=5)\n",
        "for e in range(epochs):\n",
        "  print(\"\\n/////          \" + str(e) + \"      ////////    \")\n",
        "  denoise_history = denoise_model.fit_generator(generator=denoise_generator, \n",
        "                                                epochs=1, verbose=1, \n",
        "                                                validation_data=denoise_generator_val)\n",
        "  denoise_log.append(denoise_history)\n",
        "  ### Saves optimizer and weights\n",
        "  denoise_model.save('denoise.h5') \n",
        "  ### Uploads files to external hosting\n",
        "  !curl -F \"file=@denoise.h5\" https://file.io\n",
        "    \n",
        "del denoise_generator\n",
        "del denoise_generator_val"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LMWKfDEnyY9f",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Train Descriptor"
      ]
    },
    {
      "metadata": {
        "id": "vVx9FWnKyZXk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Lambda\n",
        "shape = (32, 32, 1)\n",
        "xa = Input(shape=shape, name='a')\n",
        "xp = Input(shape=shape, name='p')\n",
        "xn = Input(shape=shape, name='n')\n",
        "descriptor_model = get_BL_descriptor_model(shape)\n",
        "ea = descriptor_model(xa)\n",
        "ep = descriptor_model(xp)\n",
        "en = descriptor_model(xn)\n",
        "\n",
        "loss = Lambda(triplet_loss)([ea, ep, en])\n",
        "\n",
        "desc_log = []\n",
        "\n",
        "descriptor_model_trip = Model(inputs=[xa, xp, xn], outputs=loss)\n",
        "# sgd = keras.optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=0.0)\n",
        "adam = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
        "descriptor_model_trip.compile(loss='mean_absolute_error', optimizer=adam, metrics=['acc'])\n",
        "\n",
        "### Descriptor loading and training\n",
        "# Loading images\n",
        "hPatches = HPatches(train_fnames=train_fnames, test_fnames=test_fnames,\n",
        "                    denoise_model=None, use_clean=True)\n",
        "# Creating training generator\n",
        "training_generator = DataGeneratorDesc(*hPatches.read_image_file(hpatches_dir, train=1), num_triplets=100000)\n",
        "# Creating validation generator\n",
        "val_generator = DataGeneratorDesc(*hPatches.read_image_file(hpatches_dir, train=0), num_triplets=10000)\n",
        "plot_triplet(training_generator)\n",
        "\n",
        "epochs = 50\n",
        "\n",
        "for e in range(epochs):\n",
        "  print(\"\\n/////          \" + str(e) + \"      ////////    \")\n",
        "  descriptor_history = descriptor_model_trip.fit_generator(generator=training_generator, epochs=1, verbose=1, validation_data=val_generator)\n",
        "  \n",
        "  desc_log.append(descriptor_history)\n",
        "  \n",
        "  ### Saves optimizer and weights\n",
        "  descriptor_model_trip.save('descriptor.h5') \n",
        "  ### Uploads files to external hosting\n",
        "  !curl -F \"file=@descriptor.h5\" https://file.io"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iqxFi79W1PZq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Run benchmark"
      ]
    },
    {
      "metadata": {
        "id": "UZ97yL-c0VcG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "generate_desc_csv(descriptor_model, seqs_test, denoise_model=denoise_model, use_clean=False)\n",
        "\n",
        "!python ./hpatches-benchmark/hpatches_eval.py --descr-name=custom --descr-dir=/content/DL_CW_HPatches/out/ --task=verification --delimiter=\";\"\n",
        "!python ./hpatches-benchmark/hpatches_results.py --descr=custom --results-dir=./hpatches-benchmark/results/ --task=verification\n",
        "\n",
        "!python ./hpatches-benchmark/hpatches_eval.py --descr-name=custom --descr-dir=/content/DL_CW_HPatches/out/ --task=matching --delimiter=\";\"\n",
        "!python ./hpatches-benchmark/hpatches_results.py --descr=custom --results-dir=./hpatches-benchmark/results/ --task=matching\n",
        "\n",
        "!python ./hpatches-benchmark/hpatches_eval.py --descr-name=custom --descr-dir=/content/DL_CW_HPatches/out/ --task=retrieval --delimiter=\";\"\n",
        "!python ./hpatches-benchmark/hpatches_results.py --descr=custom --results-dir=./hpatches-benchmark/results/ --task=retrieval"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}